# 1. DPC（Defect Pixel Correction，坏点校正）
## 坏点简介
图像坏点(Bad pixel) : 图像传感器上光线采集点(像素点)所形成的阵列存在工艺上的缺陷，或光信号进行转化为电信号的过程中出现错误，从而会造成图像上像素信息错误，导致图像中的像素值不准确，这些有缺陷的像素即为图像坏点。
        由于来自不同工艺技术和传感器制造商，尤其对一些低成本、消费品的sensor来说，坏点数会有很多。另外，sensor在长时间、高温环境下坏点也会越来越多，从而破坏了图像的清晰度和完整性。坏点校正的目的就是修复这类问题，通常坏点分为一下两种：

 (1) 静态坏点：分为静态亮点和静态暗点。

静态亮点：一般来说像素点的亮度值是正比于入射光的，而亮点的亮度值明显大于入射光乘以相应比例，并且随着曝光时间的增加，该点的亮度会显著增加；

静态坏点：无论在什么入射光下，该点的值接近于0;

(2) 动态坏点：在一定像素范围内，该点表现正常，而超过这一范围，该点表现的比周围像素要亮。与sensor 温度、增益有关，sensor 温度升高或者gain 值增大时，动态坏点会变的更加明显；

## 坏点校正策略
图像的坏点校正(DPC)通常在Bayer域(灰度图原理一致)进行。若Bayer域为R/G/B三通道，则分别进行坏点校正;若Bayer域为RGBIR格式，则分别对R/Gr/Gb/B四通道独立进行。动态坏点校正和静态坏点校正是两个相互独立的过程，可以同时开启，也可以只开启一个，视需要设置。

- 静态坏点校正：基于已有的静态坏点表，比较当前点的坐标是否与静态坏点表中的某个坐标一致，若一致则判定为坏点，然后再计算校正结果对其进行校正。一般情况下，每个sensor的坏点都不一样，需要sensor厂商给出每个sensor的静态坏点表，但是出于成本的考虑，很多sensor厂商并没有给出，而用户校正的话只能一个一个对其进行校正，因此对于一些低成本的sensor，静态坏点校正的实用性不是很强。另外，由于在硬件设计的时候需要占用大量的memory，考虑到芯片面积以及一些其他原因，因此静态坏点有大小的限制，不可以无限制的校正。

- 动态坏点校正：可以实时的检测和校正sensor 的亮点与暗点，并且校正的坏点个数不受限制。动态坏点校正相对静态坏点校正具有更大的不确定性。动态dpc可以分为两个步骤，分别为坏点检测和坏点校正。

## 算法实现


# 2. 黑电平校正
**黑电平校正**：为什么要黑电平校正，1.sensor的ADC精度和分辨力问题，这两项，导致绝对黑电平时，在允许范围内会有偏差；2.暗电流问题，工艺不可能保证每个像元的特性一致，在不同的温度和曝光时长下，响应也是不同的，所以在不同情况下，即使处于无光照的环境中，像素也会有一定的输出电压；如图是黑电平校正效果

![黑电平校正](https://upload-images.jianshu.io/upload_images/23495115-4fc50283d049d375?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

校正前需要根据图像的具体情况进行分析，若图像平面趋于平整，则推荐使用全帧均值；若图像出现一些峰值，有明显突出山峰等，推荐使用中值的方法；若出现某个角的值比较高，可能由于电源或者其他的原因引起的，则推荐使用局部计算的方法；此外，还有自定义、最大值等方法，需要根据不同图像的情况去选择方法，终其目的都是一样的。

也可以使用简单的方法，直接减去黑电平值，校准到0值，这种方法鲁棒性不好，但是简单，节省硬件资源；也可以根据温度和增益的变化实时校正，提前根据特性数据点建立一次函数，在拉流过程中进行曲线校正；


# 3. FPNC（Fixed Pattern Noise Correction，固定模式噪声校正，又称暗场校正）
FPN噪声(Fixed Pattern Noise)简称固定模式噪声，根据FPN噪声形成机制，分为行FPN和列FPN。行FPN： 在基于模拟域累加实现的TDI-CMOS图像传感器中。由于模拟累加器电路中存在寄 生电阻和电容，电路失配会导致输出图像在TDI(时间延迟积分)扫描方向(即“沿轨”方向) 亮度不均匀，且呈周期性衰减，突出表现为周期性横条纹。列FPN：传感器列并行读出电路(模拟累加器和ADC(模数转换器)等)的系统结构由于 工艺偏差很容易出现列与列之间的失配，从而导致输出图像在与TDI扫描方向垂直的方向 (即“跨轨”方向)亮度不均匀，表现为明暗变化的竖条纹。在TDI-CMOS图像传感器的输出图像中，由行FPN导致的横条纹和由列FPN导致的 竖条纹同时存在且交织在一起，如下图所示：
![](图像预处理校正简介.assets\23495115-ef9a2415e62436a2.png)
**FPN噪声计算**
FPN噪声衡量通常在极低照环境下进行，即将相机放置极低照环境(或关闭镜头光圈，让sensor采集黑帧)，设置多个曝光时间，每个曝光时间采集30张黑帧图片。FPN噪声强度用标准差和均值进行衡量，具体实现如下： 
```
exp_time = [0.063, 1.003,16, 64,257,513,770,1027,1283,1540,1797,2054];
raw_avg = 0;
for kk = 0:30:(30*12-1)
   for i = 1:30
       fname = fileNames{kk+i};
       fprintf('processing %s %d\n', fname, kk+i);
       raw = double(imread([fold fname]));
       raw = raw(:,:,1);
       raw_avg = raw + raw_avg;
   end
   raw_avg = raw_avg./30;

   avg_signal((kk/30)+1) = round(mean2(raw_avg)); %FPN均值
   fpn_total((kk/30)+1) = std2(raw_avg);  %FPN标准差

   fpn_col_exp((kk/30)+1) = std(mean(raw_avg,1)); %列FPN均值
   fpn_row_exp((kk/30)+1)     = std(mean(raw_avg,2)'); %行FPN均值
end
```
如上计算，可以得到图像的平均信号，每个曝光的FPN noise，以及行，列FPN noise,行列均值，变化如下：
![](图像预处理校正简介.assets\23495115-7938c1c76eb5e200.png)

## 算法实现

# 4. PRNUC（Photo Response Non-Uniformity Correction，图像非均匀性响应校正，又称明场校正）
主要是由于图像传感器光响应不均匀，造成图像在亮场下亮度显示不均匀

## 算法实现


# 5. 平场校正(FFC)：原因，光线不均匀、镜头中心和边缘响应不一致，sensor固定噪声和响应不均匀噪声等。

平场校正一般用FPN暗场校正和PRNU明场校正进行，平场校正就是以整帧图像的均值或者中值为目标图像，根据每个像素的特性不同，以响应增益gain为系数K和offset偏置B，对每个像素进行校正，从而使得整幅图像看起来很均匀平滑，当然，为了简化计算，节省资源，也可以使用多个相邻的像素使用同一组校准参数。

校正时，可以采用多段校正法将响应曲线分段校正，而更多的是简单暴力的两点校正法，也就是看成线性响应。
- 在暗场校正中，可以得到均值Vavgb，像素值Vinb；
- 在明场校正中，可以得到均值Vavgr，像素值Vinr；
- 可以得到增益响应系数K=(Vavgr-Vavgb)/(Vinr-Vinb)；
- 偏置B=Vavgb-Vinb*K；
- 将n个像素的每一个像素点的K[n]和B[n]写入一个ram表中，当读取一帧图像时，对每一个像素进行校正Vout=Vin*K+B;

## 算法实现


# 6. LSC（Lens Shadow Correction，镜头阴影校正）
转载自[https://blog.csdn.net/xiaoyouck/article/details/77206505](https://blog.csdn.net/xiaoyouck/article/details/77206505)

## 介绍
镜头阴影校正（Lens Shading Correction）是为了解决由于lens的光学特性，由于镜头对于光学折射不均匀导致的镜头周围出现阴影的情况。

shading可以细分为luma shading和color shading：

*   luma shading：
    由于Lens的光学特性，Sensor影像区的边缘区域接收的光强比中心小，所造成的中心和四角亮度不一致的现象。镜头本身就是一个凸透镜，由于凸透镜原理，中心的感光必然比周边多。如图所示：
    ![](https://upload-images.jianshu.io/upload_images/23495115-b9392d757b5ca13d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

*   chrom/color shading：
    由于各种颜色的波长不同，经过了透镜的折射，折射的角度也不一样，因此会造成color shading的现象，这也是为什么太阳光经过三棱镜可以呈现彩虹的效果。如图所示：
    ![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-65ef5206150ebdb3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

此外，还有CRA的原因会导致shading现象的出现，这里不再赘述，这里推荐《[What’s CRA](http://blog.csdn.net/huddheaven/article/details/52300262)》这篇文章，详细讲述了由于镜头的CRA带来的shading。

## 影响

luma shading：会造成图像边角偏暗，就是所谓的暗角。
![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-0a61ca2bdf8a1159?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

color shading：中心和四周颜色不一致，体现出来一般为中心或者四周偏色。如图所示：
![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-106b02b0775f5e8c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 校正

lens shading的校正是分别对于bayer的四个通道进行校正，每个通道的校正过程是相对独立的过程。

考虑到芯片设计的成本，因此一般情况下不会存储整幅图像的lut，目前主流的都是存储128*128个点的增益，利用双线性插值的方法计算每个pixel的增益。

### 算法

**由于条件限制，图像仅用于算法验证，不做图像质量评判标准**
这里写了一个shading的算法，将图像分为16x16的方块，求取每个交点的增益值，对平面进行四次方拟合，分别计算了luma shading 和 chrom shading，先计算出来一个lut用于存储，校正的世行通过对这个lut进行双线性插值得到每个pixel的值乘以原本像素点。

16x16的分块并非固定，可以对块的大小进行调整，比如中心块偏大，靠近边缘的方块变小，这些都是可以自定义的，本算法由于做演示使用，故不做其他功能。如图所示：
![](https://upload-images.jianshu.io/upload_images/23495115-1e1a563812855931?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

由于代码量较大，这里分别附上一部分算法

shading lut caculate:

```
function [image_r_gain, image_gr_gain, image_gb_gain, image_b_gain] = ...
isp_lsc_lut(image_r, image_gr, image_gb, image_b, side_num)
[height, width] = size(image_r);
side_y = floor(height/side_num);
side_x = floor(width/side_num);

% figure,imshow(image_r);
% hold on;
% for k=0:side_num
%     line_x = side_x * k;
%     line_y = side_y * k;
%     if(k==side_num && line_y ~= width) line_y = height;end
%     if(k==side_num && line_x ~= width) line_x = width;end
%     line([line_x,line_x],[0,height],'Color','red');
%     line([0,width], [line_y, line_y],'Color','red');
% %     line(Xd,Yd,'Color','red');
% end
% hold off

%% compress resolution
image_point = zeros(side_num,side_num);
for i = 0:side_num
    for j = 0:side_num
        x_clip = floor([j*side_x - side_x/2, j*side_x + side_x/2]);
        y_clip = floor([i*side_y - side_y/2, i*side_y + side_y/2]);
        if(i==side_num && y_clip(2) ~= height) y_clip(2) = height;end
        if(j==side_num && x_clip(2) ~= width) x_clip(2) = width;end
        x_clip(x_clip<1) = 1;x_clip(x_clip>width) = width;
        y_clip(y_clip<1) = 1;y_clip(y_clip>height) = height;
        data_r_in = image_r(y_clip(1):y_clip(2), x_clip(1):x_clip(2));
        image_r_point(i+1,j+1) = mean(mean(data_r_in));
        data_gr_in = image_gr(y_clip(1):y_clip(2), x_clip(1):x_clip(2));
        image_gr_point(i+1,j+1) = mean(mean(data_gr_in));
        data_gb_in = image_gb(y_clip(1):y_clip(2), x_clip(1):x_clip(2));
        image_gb_point(i+1,j+1) = mean(mean(data_gb_in));
        data_b_in = image_b(y_clip(1):y_clip(2), x_clip(1):x_clip(2));
        image_b_point(i+1,j+1) = mean(mean(data_b_in));
    end
end

% figure,imshow(uint8(image_r_point));
%% caculate lsc luma gain
for i = 1:side_num+1
    for j = 1:side_num+1
        image_r_luma_gain_point(i,j) = mean2(image_r_point(uint8(side_num/2)-1:uint8(side_num/2)+1, uint8(side_num/2)-1:uint8(side_num/2)+1)) / image_r_point(i,j);
        image_gr_luma_gain_point(i,j) = mean2(image_gr_point(uint8(side_num/2)-1:uint8(side_num/2)+1, uint8(side_num/2)-1:uint8(side_num/2)+1)) / image_gr_point(i,j);
        image_gb_luma_gain_point(i,j) = mean2(image_gb_point(uint8(side_num/2)-1:uint8(side_num/2)+1, uint8(side_num/2)-1:uint8(side_num/2)+1)) / image_gb_point(i,j);
        image_b_luma_gain_point(i,j) = mean2(image_b_point(uint8(side_num/2)-1:uint8(side_num/2)+1, uint8(side_num/2)-1:uint8(side_num/2)+1)) / image_b_point(i,j);
    end
end

```

bilinear interpolation:

```
image_r_luma_gain_reshape = reshape(image_r_luma_gain_point, [], 1);
image_gr_luma_gain_reshape = reshape(image_gr_luma_gain_point, [], 1);
image_gb_luma_gain_reshape = reshape(image_gb_luma_gain_point, [], 1);
image_b_luma_gain_reshape = reshape(image_b_luma_gain_point, [], 1);
for i = 1:17
    for j = 1:17
        x((i-1)*17+j) = i;
        y((i-1)*17+j) = j;
    end
end
x=x';
y=y';
% scatter3(x,y,image_r_luma_gain_reshape)
% hold on
Z=[ones(length(x),1),x,y,x.^2,x.*y,y.^2,x.^3,x.^2.*y,x.*y.^2,y.^3];
[x y]=meshgrid(1:17,1:17);
A=Z\image_r_luma_gain_reshape;
image_r_luma_gain=A(1)+A(2)*x+A(3)*y+A(4)*x.^2+A(5)*x.*y+A(6)*y.^2+A(7)*x.^3+A(8)*x.^2.*y+A(9)*x.*y.^2+A(10)*y.^3;
A=Z\image_gr_luma_gain_reshape;
image_gr_luma_gain=A(1)+A(2)*x+A(3)*y+A(4)*x.^2+A(5)*x.*y+A(6)*y.^2+A(7)*x.^3+A(8)*x.^2.*y+A(9)*x.*y.^2+A(10)*y.^3;
A=Z\image_gb_luma_gain_reshape;
image_gb_luma_gain=A(1)+A(2)*x+A(3)*y+A(4)*x.^2+A(5)*x.*y+A(6)*y.^2+A(7)*x.^3+A(8)*x.^2.*y+A(9)*x.*y.^2+A(10)*y.^3;
A=Z\image_b_luma_gain_reshape;
image_b_luma_gain=A(1)+A(2)*x+A(3)*y+A(4)*x.^2+A(5)*x.*y+A(6)*y.^2+A(7)*x.^3+A(8)*x.^2.*y+A(9)*x.*y.^2+A(10)*y.^3;
% surf(x,y,image_r_luma_gain)
% hold on 
% surf(x,y,image_r_luma_gain_point)

%% calulate lsc chroma gain
for i = 1:side_num+1
    for j = 1:side_num+1
        image_r_chroma_gain(i,j) = image_r_luma_gain(i,j) - image_r_luma_gain_point(i,j);
        image_gr_chroma_gain(i,j) = image_gr_luma_gain(i,j) - image_gr_luma_gain_point(i,j);
        image_gb_chroma_gain(i,j) = image_gb_luma_gain(i,j) - image_gb_luma_gain_point(i,j);
        image_b_chroma_gain(i,j) = image_b_luma_gain(i,j) - image_b_luma_gain_point(i,j);
    end
end
%% caculate lsc result gain
image_r_gain = image_r_luma_gain - image_r_chroma_gain;
image_gr_gain = image_gr_luma_gain - image_gr_chroma_gain;
image_gb_gain = image_gb_luma_gain - image_gb_chroma_gain;
image_b_gain = image_b_luma_gain - image_b_chroma_gain;

function image_gain_lut = lsc_data_gain_interpolation(image_gain, height, width, side_num)
side_y_ori = floor(height/side_num);
side_x_ori = floor(width/side_num);
k = 0;
l = 0;
[gain_height, gain_width] = size(image_gain);
for i = 1:gain_height-1
    for j = 1:gain_width-1
        data_gain_11 = image_gain(i, j);
        data_gain_12 = image_gain(i, j+1);
        data_gain_21 = image_gain(i+1, j);
        data_gain_22 = image_gain(i+1, j+1);
        if(j == gain_width-1 && ((j-1)*side_x + l) ~= width) 
            side_x = width - (j-1)*side_x_ori;
        else
            side_x = side_x_ori;
        end

        if(i == gain_width-1 && ((i-1)*side_y + k) ~= width)
            side_y = height - (i-1)*side_y_ori;
        else
            side_y = side_y_ori;
        end

        for k = 1:side_y
            for l = 1:side_x
                label_y1 = 1;
                label_x1 = 1;
                label_y2 = side_y;
                label_x2 = side_x;
                image_gain_lut((i-1)*side_y_ori + k, (j-1)*side_x_ori + l) = ...
                    data_gain_22/(label_x2-label_x1)/(label_y2-label_y1)* ...
                    (l - label_x1) * (k - label_y1) + ...
                    data_gain_21/(label_x2-label_x1)/(label_y2-label_y1)* ...
                    (label_x2 - l) * (k - label_y1) + ...
                    data_gain_12/(label_x2-label_x1)/(label_y2-label_y1)* ...
                    (l - label_x1) * (label_y2 - k) + ...
                    data_gain_11/(label_x2-label_x1)/(label_y2-label_y1)* ...
                    (label_x2 - l) * (label_y2 - k);
            end
        end

    end
end
end

```

#### 效果展示：

**实验条件有限，图片有水波纹，仅用于理解算法**

original image:
![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-50b729648d618e33?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

luma shading

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-18efc94e1377df61?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-2a37bb479b039b45?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-6f6a7a3dffc2f758?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

chroma shading
![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-8de01a882912df14?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-1e9e60aa61e22118?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-5040c4115b1c199f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

luma shading + chroma shading:
![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-39aee87c4823148c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-0f7f516a85b502df?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![这里写图片描述](https://upload-images.jianshu.io/upload_images/23495115-1617bffe75b3fdb0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### tuning

LSC的tuning一定要把校正图采集好，一般情况下raw图的G通道中心亮度在8bit的70%~80%之间，由于在不同色温情况下是经过插值的，因此需要校正多个光源，一般情况下TL84、D65、A光源下进行校正。将得到的LUT写入RAM中即可
**注意：**采集的raw图不要有filcker。

LSC强度一般是可调的，由于图像边角的增益会很大，因此在高倍gain下，可以把强度给降低，防止图像边角噪声压不住的情况。

# 7. Gamma校正
RGB值与功率并非简单的线性关系，而是幂函数关系，这个函数的指数称为Gamma值，一般为2.2，而这个换算过程，称为Gamma校正。

**为什么显示器要Gamma校正呢？**因为人眼对亮度的感知和物理功率不成正比，而是幂函数的关系，这个函数的指数通常为2.2，称为Gamma值。

打个比方，功率为50%的灰色，人眼实际感知亮度为

![](图像预处理校正简介.assets\23495115-c452acdeacebfc82.png)


而人眼认为的50%中灰色，实际功率为

![](图像预处理校正简介.assets\23495115-b93a6012a36d9d25.png)


所以RGB中的灰度值，**为了考虑到较小的存储范围(0~255)和较平衡的亮暗部比例**，所以需要进行Gamma校正，而不是直接对应功率值，**因此RGB值RGB颜色值不能简单直接相加，而是必须用2.2次方换算成物理光功率后才能进行下一步计算。**这一点在下面的灰度计算公式中就有所体现。

在电视和图形监视器中，[显像管](http://baike.baidu.com/view/322825.htm)发生的电子束及其生成的图像亮度并不是随显像管的输入电压线性变化，电子流与输入电压相比是按照指数曲线变化的，输入电压的指数要大于[电子束](http://baike.baidu.com/subview/907731/907731.htm)的指数。这说明暗区的信号要比实际情况更暗，而亮区要比实际情况更高。所以，要重现摄像机拍摄的画面，电视和监视器必须进行伽玛补偿。这种伽玛校正也可以由摄像机完成。我们对整个电视系统进行伽玛补偿的目的，是使摄像机根据入射光亮度与显像管的亮度对称而产生的输出信号，所以应对图像信号引入一个相反的[非线性失真](http://baike.baidu.com/subview/423482/423482.htm)，即与电视系统的伽玛曲线对应的摄像机伽玛曲线，它的值应为1/γ，我们称为摄像机的[伽玛值](http://baike.baidu.com/subview/399194/399194.htm)。电视系统的伽玛值约为2.2，所以电视系统的摄像机非线性补偿伽玛值为0.45。[彩色显像管](http://baike.baidu.com/subview/2617856/2617856.htm)的伽玛值为2.8，它的图像信号校正指数应为1/2.8=0.35，但由于[显像管](http://baike.baidu.com/subview/322825/322825.htm)内外[杂散光](http://baike.baidu.com/subview/1142133/1142133.htm)的影响，重现图像的对比度和[饱和度](http://baike.baidu.com/view/189644.htm)均有所降低，所以[彩色摄像机](http://baike.baidu.com/subview/2003336/2003336.htm)的伽玛值仍多采用0.45。在实际应用中，我们可以根据实际情况在一定范围内调整伽玛值，以获得最佳效果。
![](https://upload-images.jianshu.io/upload_images/23495115-8122ce03c17279f2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**gamma校正原理：**

*Gamma correction is a basic module in ISP pipeline. In this module, the brightness of the image will be adjusted by performing curve mapping in RGB domain. In general, this module implement two functions through curve mapping, which is bit-width compression and gamma transformation.*
*adjust the contrast association with differently light levels differently to increase the salience of features. Gamma correction is crucial for viewing applications. Without it, higher image bit depth would be required to avoid visible posterization effects. The contrast of shadow details is accenntuated, but contrast in highlisghts is conpressed.*
由于灰阶有限，为了能够让人感受到每个灰阶的变化，如何采用有限的灰阶表示物理世界的光照强度问题，人眼对于物理光照的变化并不是线性的，对于低光照下灰阶的变化更为敏感，所以Gamma曲线将低光照下的数据范围拉伸，高光照数据范围压缩，让人眼能够感受到更多的灰阶。
所以一般Gamma开启后图像会变得比未开启下更亮。

　　假设图像中有一个像素，值是 200 ，那么对这个像素进行校正必须执行如下步骤： 
　　1\. 归一化 ：将像素值转换为  0 ～ 1  之间的实数。 算法如下 : ( i + 0\. 5)/256  这里包含 1 个除法和 1 个加法操作。对于像素  A  而言  , 其对应的归一化值为  0\. 783203 。 

　　2\. 预补偿 ：根据公式  , 求出像素归一化后的 数据以  1 /gamma  为指数的对应值。这一步包含一个 求指数运算。若  gamma  值为  2\. 2 ,  则  1 /gamma  为  0\. 454545 , 对归一化后的  A  值进行预补偿的结果就 是  0\. 783203 ^0\. 454545 = 0\. 894872 。 

　　3\. 反归一化 ：将经过预补偿的实数值反变换为  0  ～  255  之间的整数值。具体算法为 : f*256 - 0\. 5  此步骤包含一个乘法和一个减法运算。续前 例  , 将  A  的预补偿结果  0\. 894872  代入上式  , 得到  A  预补偿后对应的像素值为  228 , 这个  228  就是最后送 入显示器的数据。

　　如上所述如果直接按公式编程的话，假设图像的分辨率为 800*600 ，对它进行 gamma 校正，需要执行 48 万个浮点数乘法、除法和指数运算。效率太低，根本达不到实时的效果。 
　　针对上述情况，提出了一种快速算法，如果能够确知图像的像素取值范围  , 例如  , 0 ～ 255 之间的整数  , 则图像中任何一个像素值只能 是  0  到  255  这  256  个整数中的某一个 ; 在  gamma 值 已知的情况下  ,0 ～ 255  之间的任一整数  , 经过“归一 化、预补偿、反归一化”操作后 , 所对应的结果是唯一的  , 并且也落在  0 ～ 255  这个范围内。
　　如前例  , 已知  gamma  值为  2\. 2 , 像素  A  的原始值是  200 , 就可求得 经  gamma  校正后  A  对应的预补偿值为  228 。基于上述原理  , 我们只需为  0 ～ 255  之间的每个整数执行一次预补偿操作  , 将其对应的预补偿值存入一个预先建立的  gamma  校正查找表 (LUT:Look Up Table) , 就可以使用该表对任何像素值在  0 ～ 255  之 间的图像进行  gamma  校正。

**Gamma校正实现：**

```

#include <iostream>  
#include <opencv2\core\core.hpp>  
#include <opencv2\highgui\highgui.hpp>  
#include <opencv2\imgproc\imgproc.hpp>  
#include <cmath>
using namespace cv;
 
Mat gammaTransform(Mat &srcImage, float kFactor)
{
	
	unsigned char LUT[256];
	for (int i = 0; i < 256; i++)
	{
		float f = (i + 0.5f) / 255;
		f = (float)(pow(f, kFactor));
		LUT[i] = saturate_cast<uchar>(f*255.0f - 0.5f);
	}
	Mat resultImage = srcImage.clone();
	
	if (srcImage.channels() == 1)
	{
		
		MatIterator_<uchar> iterator = resultImage.begin<uchar>();
		MatIterator_<uchar> iteratorEnd = resultImage.end<uchar>();
		for (; iterator != iteratorEnd; iterator++)
		{
			*iterator = LUT[(*iterator)];
		}
	}
	else
	{
		
		
		MatIterator_<Vec3b> iterator = resultImage.begin<Vec3b>();
		MatIterator_<Vec3b> iteratorEnd = resultImage.end<Vec3b>();
		for (; iterator != iteratorEnd; iterator++)
		{
			(*iterator)[0] = LUT[((*iterator)[0])];//b
			(*iterator)[1] = LUT[((*iterator)[1])];//g
			(*iterator)[2] = LUT[((*iterator)[2])];//r
		}
	}
	return resultImage;
}
int main()
{
	Mat srcImage = imread("lakeWater.jpg");
	if (!srcImage.data)
	{
		printf("could not load image...\n");
		return -1;
	}
	//取两种不同的gamma值
	float gamma1 = 3.33f;
	float gamma2 = 0.33f;
	float kFactor1 = 1 / gamma1;
	float kFactor2 = 1 / gamma2;
	Mat result1 = gammaTransform(srcImage, kFactor1);
	Mat result2 = gammaTransform(srcImage, kFactor2);
	imshow("srcImage", srcImage);
	imshow("res1", result1);
	imshow("res2", result2);
	waitKey(0);
	return 0;
}
```

原图：

![image](https://upload-images.jianshu.io/upload_images/23495115-9e5bc36fdfdee643?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 

gamma=3.33的效果图：

![image](https://upload-images.jianshu.io/upload_images/23495115-de125109aafb26b6?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 

Gamma=0.33的效果图：

![image](https://upload-images.jianshu.io/upload_images/23495115-0d390d79c8f06c98?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


# 随便乱记
* 当感光元件像素的空间频率与影像中条纹的空间频率接近或当图案的细条状结构与传感器的结构以小角度交叉时，摩尔纹可能出现
* 边缘插值容易出现拉链效应，因为边缘上下差异较大，使用周围分量插值时当前行插出的结果与当前行原始的结果相差较大，例如上50下100当前行90，插出82.5和90有色差。解决办法就是沿着边沿的方向进行插值。
* 人眼对于亮度更加敏感，认为在一个较小的范围内，物体颜色保持不变，抑制边缘的false color，抑制色差
* 边缘方向插值及false color 抑制方法
判断edge？N（双线性插值或无方向插值）Y-> 能否判断出边缘方向 N（无方向插值+false color抑制）Y-> 沿边缘方向插值+false color抑制
* Demosaic算法效果关注细条纹抑制效果
* AWBGain CCM Gamma LSC HDR Sharpen 都会引入噪声（类似与乘系数的运算）

