1. 在DC flow下，发现综合出来的 clock_network功耗特别大，高达300mW，功耗占比98%，综合会报TIM-134 Warning 
`Design ‘Test’ containts 2 high-fanout nets. A fanout number of 1000 will be used for delay calculations involving these nets. (TIM-134)`
分析思路：
`create_clock` 命令建立的时钟，本身被认为是ideal_network，不计算功耗，但综合时如果使用clock_gating，门控时钟之后的时钟网络的功耗会包括在最后power report里面，如果大量的寄存器使用同一个时钟门控，就会导致时钟net的fanout过大，DC默认的high_fanout_net_pin_capacitance 默认值为1，high-fanout nets 使用该值作为单个fanout的pin capacitance，导致在计算clock_network的switch power时过大（αCV2F）
解决方式：
我们修改 high_fanout_net_pin_capacitance为一个合理的值，例如 0.002-0.004.重新综合后，得到 switching power 为 3mW.
`set_app_var high_fanout_net_pin_capacitance 0.002`
实际上因为DC 在非 -topo 模式下没有实际的RC模型，所以计算都是基于wire load model进行延迟评估的，不需要mikyway等包含物理信息的文件，所以才会出现clock network非常大的情况，而在-topo模式下会有实际的 RC数据，计算会更加准确
结论：
* 如果不适用门控时钟，则clock net本身是ideal_network，power report里面功耗为0，不会产生该问题
* 如果门控时钟net的fanout不大，小于high_fanout_net_threshold，也不会产生该问题
* 在大型设计中，由于整体功耗偏大，该问题产生的多余功耗可能因为不明显而不被发现
* DC对于high fanout的处理基本就是加buffer，以此来减少cell输出端的负载，从而减少transation time 和 delay time， 以及max_capacitance，但实际上并不希望dc插入buffer，而是在后端设计是插入，因为我们不知道真实的high fanout net上面的RC，所以不知道应该怎么样插入buffer，dc只是根据互连线模型来计算RC，只有布线之后才能得到近乎真是的RC。所以在dc综合过程中我们要阻止dc最high fanout net进行insert buffer处理。因此这些没被处理的高扇出net就会引起一些drc或者timing错误，在dc中，dc用价值函数（cost function）来判断这些约束对设计的影响。价值函数=DRC violator和+ timing violator 和。一般的，dc会根据所有drc和timing错误，通过使价值函数趋近等于0来修正这些违规。为了达到效果，dc会每次修正一个路径，然后重新计算价值函数，如果价值函数变小，说明设计被改进了。所以为了避免插入buffer，clock 需要 create_clock，reset需要 set_ideal_network（待定），一般信号手动设置 set_ideal_network
* 综合的时候multi_cycle不能随便设置，一定要根据实际的逻辑波形去确定可以设置multi_cycle了再去设置
* 综合是先elaborate然后再map，sdc是在两个之间加入的，所以在设置约束的时候一定要保证约束到的目标是存在的，不会因为map或者优化而改变。例如设置约束到dff的D端，因为map之后就是stdcell，不再是RTL内部的命名，会导致map之后sdc设置找不到目标从而约束不上，所以要确定elaborate之后的端口名称，例如dff的D端口，elaborate之后叫做next_state，map之后叫做D，因为是在elaborate和map之间，所以约束应该在next_state上，最后map输出的sdc也会正常转换到D端口
* IO异步情况下想设置max_delay的时候可以优先考虑通过创建虚拟时钟然后加上input output delay的方式去约束
* create_generated_clock 的-source后面跟的是clock_source_pin，因此只能跟port或者pin
* 使用DC报告slack较差的路径的前后级slack，便于判断是否能够向前后级借timing
```
proc custom_report_worst_path_per_group { } {
    echo [format "%-20s %-20s %-20s %-20s %-20s %7s" "From" "To" "Slack" "Next_Stage_From" "Next_Stage_To" "Next_Stage_Slack"]
    echo "-----------------------------------------------------------------------------------------------------------------------------"
    foreach_in_collection path [get_timing_paths -nworst 10 -slack_lesser_than 0.2] {
       set slack [get_attribute $path slack]
       set startpoint [get_attribute $path startpoint]
       set endpoint [get_attribute $path endpoint]
       set next_stage_startpoint [get_pins -of_objects [get_cells -of_objects [get_pins $endpoint]] -filter "name =~ CK"]
       set next_stage_path [get_timing_paths -nworst 1 -from $next_stage_startpoint]
       set next_stage_endpoint [get_attribute $next_stage_path endpoint]
       set next_stage_slack [get_attribute $next_stage_path slack]
       # Also can add pre stage
       set pre_stage_endpoint [get_pins -of_objects [get_cells -of_objects [get_pins $startpoint]] -filter "name =~ D"]
       set pre_stage_path [get_timing_paths -nworst 1 -to $pre_stage_endpoint]
       set next_stage_startpoint [get_attribute $next_stage_path startpoint]
       set next_stage_slack [get_attribute $next_stage_path slack]
       echo [format "%-20s %-20s %s %-20s %-20s %s" [get_attribute $startpoint full_name] [get_attribute $endpoint full_name] $slack [get_attribute $next_stage_startpoint full_name] [get_attribute $next_stage_endpoint full_name] $next_stage_slack]
    }
}

```
